{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMLDL-desafioFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bYm8kxh8ktZl",
        "o_msWTUDmfLy",
        "a1qNMXovbl7r",
        "p-bQE2J2QZcS",
        "FiZxy4ronJTI",
        "tFFPdSsDp23x",
        "jhuDWbmHq4hB",
        "uT4sLlmZsWxv",
        "TsNe0YWUfZ2X"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d362e81c33a4a9f946fc48b7f40e04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a424ad4ed20a47fc948f2e8642288fe7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d6e07adb1aa40579f68a43310d908eb",
              "IPY_MODEL_1254a7235d2d49929ce2fbf993223a47"
            ]
          }
        },
        "a424ad4ed20a47fc948f2e8642288fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d6e07adb1aa40579f68a43310d908eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5fb207a31b0f4a5385dcdbe67a47a541",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29194e4b97204342be6cd3a2cb40724e"
          }
        },
        "1254a7235d2d49929ce2fbf993223a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec9a2e6283b444f1b5d8c5fb1b98fbb9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:03&lt;00:00, 79.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_015b2b7c44cb4f4d9f58d3f69f9a2fcd"
          }
        },
        "5fb207a31b0f4a5385dcdbe67a47a541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29194e4b97204342be6cd3a2cb40724e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec9a2e6283b444f1b5d8c5fb1b98fbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "015b2b7c44cb4f4d9f58d3f69f9a2fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N65NPk0jpHA"
      },
      "source": [
        "# Desafio Final - Dupla Echo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYm8kxh8ktZl"
      },
      "source": [
        "# Introdução\n",
        "\n",
        "O desafio final consiste na implementação de um modelo classificador inteligente que seja capaz de analisar imagens do bioma pantanal e dessa forma indicar se há ou não alguma das espécies de animais que foram listadas (onça-pintada, lobo guará, ariranha, tatu canastra, tamanduá bandeira, jacaré do papo amarelo, sucuri, tucano, piranha e capivara)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0PPMSjrlWin"
      },
      "source": [
        "O primeiro passo é montar o drive, dessa forma o *Jupyter Notebook* terá acesso aos conteúdos presentes no Google Drive, possibilitando a execução do *Notebook* pela nuvem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_0s8lsxh6cH",
        "outputId": "db4fb1be-091a-4d14-b28a-09202dd8b309"
      },
      "source": [
        "# Montando o drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAK97x7ulvWS"
      },
      "source": [
        "Para a resolução do problema o time optou por usar uma Rede Neural Convolucional. Algumas bibliotecas serão utilizadas e para isso, serão importadas logo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfpaO2IQix9T"
      },
      "source": [
        "# Utilitários\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "#%matplotlib inline\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import nn\n",
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtGDEt15mHsC"
      },
      "source": [
        "Com o intuito de otimizar o processo de treinamento da rede, a célula abaixo testa se está disponível uma GPU, caso contrário será utilizada a CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO_6uzuY5gSj",
        "outputId": "d9bc16be-a065-46e4-841a-954af4da60a0"
      },
      "source": [
        "def testar_gpu():\n",
        "\ttrain_on_gpu = torch.cuda.is_available() #Observa se a GPU está disponivel\n",
        "\tif train_on_gpu: #Se sim\n",
        "\t\tdevice = torch.device('cuda') #Seleciona o device como GPU\n",
        "\t\tprint(\"Treinando na GPU\") #E manda a mensagem\n",
        "\telse: #Se não\n",
        "\t\tdevice = torch.device('cpu') #Seleciona o device como cpu\n",
        "\t\tprint(\"GPU indisponível, treinando na CPU\") #E avisa que a GPU não esta disponível\n",
        "\treturn device\n",
        "\n",
        "device = testar_gpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treinando na GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_msWTUDmfLy"
      },
      "source": [
        "# Criação do Dataset com FastAi\n",
        "\n",
        "O FastAI é uma biblioteca de Machine Learning que será utilizada no problema com o intuito de facilitar a criação do dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqT1lvn6jJJO",
        "outputId": "3818fe36-1053-4556-bb4c-7be358313eb5"
      },
      "source": [
        "# Instalamos a bilioteca para a utilização no Notebook\n",
        "!pip install git+https://github.com/fastai/fastai.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/fastai/fastai.git\n",
            "  Cloning https://github.com/fastai/fastai.git to /tmp/pip-req-build-ssetpgtd\n",
            "  Running command git clone -q https://github.com/fastai/fastai.git /tmp/pip-req-build-ssetpgtd\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (19.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (20.9)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (0.9.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (3.13)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (2.2.4)\n",
            "Requirement already satisfied: torch<1.9,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.3.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai==2.3.1) (2.4.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.2->fastai==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.3.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.3.1) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==2.3.1) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.3.1) (2020.12.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (56.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.3.1) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9,>=1.7.0->fastai==2.3.1) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.3.1) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.3.1) (3.4.1)\n",
            "Building wheels for collected packages: fastai\n",
            "  Building wheel for fastai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastai: filename=fastai-2.3.1-cp37-none-any.whl size=193488 sha256=3e2a3f105d92c4f382370d1409b1c7f07d18ce42161c66a9819fb65fbffe1584\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u8y8b0mg/wheels/cf/46/39/b2d08762125ed2376861976ab2c4ac30c029b86e375735d9b8\n",
            "Successfully built fastai\n",
            "Installing collected packages: fastcore, fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.3.1 fastcore-1.3.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1-nyK9m320"
      },
      "source": [
        "A importação da biblioteca FastAI é realizada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZQNY__4j8-G"
      },
      "source": [
        "import fastai.vision.all as fst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IsOrvL2a96O"
      },
      "source": [
        "Definimos as duas variáveis *dataset_path* e *destionation_path* que irão armazenar o diretório com os arquivos csv e as imagens, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "069iKS_gk_1p",
        "outputId": "94790b8e-5e41-4a3d-9788-ba0ab028c689"
      },
      "source": [
        "\n",
        "dataset_path = '/content/drive/My Drive/IMLDL/IMLDL-Desafio-DataSet'\n",
        "destination_path = '/content/drive/My Drive/IMLDL/IMLDL-Desafio-Imagens'\n",
        "\n",
        "animals = ['ariranha', 'capivara', 'jacaré-papo-amarelo', 'lobo-guará', 'onça-pintada', 'piranha', 'sucuri', 'tamanduá-bandeira',\n",
        "           'tatu-canastra', 'tucano', 'pantanal']\n",
        "\n",
        "!ls '/content/drive/My Drive/IMLDL/IMLDL-Desafio-DataSet'\n",
        "!ls '/content/drive/My Drive/IMLDL/IMLDL-Desafio-Imagens'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ariranha.csv\t\t onça-pintada.csv  tamanduá-bandeira.csv\n",
            "capivara.csv\t\t pantanal.csv\t   tatu-canastra.csv\n",
            "jacaré-papo-amarelo.csv  piranha.csv\t   tucano.csv\n",
            "lobo-guará.csv\t\t sucuri.csv\n",
            "ariranha  jacaré-papo-amarelo  onça-pintada  piranha  tamanduá-bandeira  tucano\n",
            "capivara  lobo-guará\t       pantanal      sucuri   tatu-canastra\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1qNMXovbl7r"
      },
      "source": [
        "## Download das imagens\n",
        "As células abaixo irão ler os arquivos csv e baixar as imagens para o jupyter notebook. Apenas rode se você não possuir as imagens no seu drive!\n",
        "\n",
        "O número de imagens baixadas é definido pela variável *Num_pics*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLjwnhSdo9B9"
      },
      "source": [
        "# Número de imagens a serem baixadas\n",
        "Num_pics = 300\n",
        "\n",
        "# Caminhos de destino para o .csv e pasta de cada animal\n",
        "csv_paths = [fst.Path(dataset_path + '/' + animal + '.csv') for animal in animals] # fst.Path(dataset_path)\n",
        "dest_paths = [fst.Path(destination_path + '/' + animal) for animal in animals] # fst.Path(dest_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcyv3fdyyI4F",
        "outputId": "85e82d3c-75f8-4fff-bf44-40eb594b0d6a"
      },
      "source": [
        "print(dest_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/IMLDL/IMLDL-Desafio-Imagens/capivara\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YFd7jY2k2B-",
        "outputId": "633b5700-deb8-4ce5-a217-005a3d7364bb"
      },
      "source": [
        "files = {}\n",
        "for i, animal in enumerate(animals):\n",
        "  print('Reading: ', animal)\n",
        "  fst.download_images(dest_paths[i], csv_paths[i], max_pics=Num_pics)\n",
        "  for _,_,filenames in os.walk(dest_paths[i]):\n",
        "    # print(filenames)\n",
        "    files[animal] = [str(dest_paths[i]) + '/' + file for file in filenames]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading:  ariranha\n",
            "Reading:  capivara\n",
            "Reading:  jacaré-papo-amarelo\n",
            "Reading:  lobo-guará\n",
            "Reading:  onça-pintada\n",
            "Reading:  piranha\n",
            "Reading:  sucuri\n",
            "Reading:  tamanduá-bandeira\n",
            "Reading:  tatu-canastra\n",
            "Reading:  tucano\n",
            "Reading:  pantanal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAofsYxasxG3",
        "outputId": "98423c13-ab45-477a-f6f3-3a3d0a1a7d26"
      },
      "source": [
        "for animal in animals:\n",
        "  print('Imagens para ', animal + ':', end=' ')\n",
        "  dir = '/content/drive/My Drive/IMLDL/IMLDL-Desafio-Imagens/' + animal\n",
        "  print(len(os.listdir(dir)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imagens para  ariranha: 277\n",
            "Imagens para  capivara: 272\n",
            "Imagens para  jacaré-papo-amarelo: 272\n",
            "Imagens para  lobo-guará: 262\n",
            "Imagens para  onça-pintada: 188\n",
            "Imagens para  piranha: 236\n",
            "Imagens para  sucuri: 239\n",
            "Imagens para  tamanduá-bandeira: 250\n",
            "Imagens para  tatu-canastra: 171\n",
            "Imagens para  tucano: 215\n",
            "Imagens para  pantanal: 190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-bQE2J2QZcS"
      },
      "source": [
        "## Criação do datset\n",
        "Definimos as transformadas que serão aplicadas durante a criação do dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6aNm3U412sF"
      },
      "source": [
        "# Processing data\n",
        "img_shape = (224, 224)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(img_shape),\n",
        "                                transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n",
        "                                transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d7iK9pAD87rE",
        "outputId": "0be41073-c288-4604-846a-ca5c0b663a89"
      },
      "source": [
        "destination_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/IMLDL/IMLDL-Desafio-Imagens'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnPdCdOd3Hlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e877bd8-eb13-4e6b-bddf-293698a4d184"
      },
      "source": [
        "# Utilzamos a função ImageFolder para criar o dataset ja com o label correto definido pela pasta\n",
        "data = ImageFolder(destination_path, transform=transform)\n",
        "data.class_to_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ariranha': 0,\n",
              " 'capivara': 1,\n",
              " 'jacaré-papo-amarelo': 2,\n",
              " 'lobo-guará': 3,\n",
              " 'onça-pintada': 4,\n",
              " 'pantanal': 5,\n",
              " 'piranha': 6,\n",
              " 'sucuri': 7,\n",
              " 'tamanduá-bandeira': 8,\n",
              " 'tatu-canastra': 9,\n",
              " 'tucano': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWLMeBmK4_dA",
        "outputId": "037831b3-7bf0-473e-e4ab-fb5c47dcf983"
      },
      "source": [
        "print('Total de imagens no dataset:', len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de imagens no dataset: 2572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9T7uDJX3Ctk",
        "outputId": "4e245321-a576-4f3c-8107-cd5db5e24c72"
      },
      "source": [
        "percentage = 70\n",
        "\n",
        "n_treino = round( len(data) * (percentage/100) )\n",
        "n_teste = round( len(data) * (100 - percentage)/(2*100) )\n",
        "n_valid = len(data) - n_treino - n_teste\n",
        "\n",
        "print('nº de imagens para treino: {:}'.format(n_treino) +'; nº de imagens para teste: {:}'.format(n_teste) +'; nº de imagens para validação: {:}'.format(n_valid) + '.')\n",
        "data_train, data_test, data_valid = random_split(data, [n_treino, n_teste, n_valid], generator=torch.Generator())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nº de imagens para treino: 1800; nº de imagens para teste: 386; nº de imagens para validação: 386.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dlpA4Z3-Kc"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "loader_train, loader_test, loader_valid = DataLoader(data_train, batch_size=batch_size), DataLoader(data_test, batch_size=1), DataLoader(data_valid, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiZxy4ronJTI"
      },
      "source": [
        "# Arquitetura da CNN\n",
        "\n",
        "O time optou por utilizar a técnica de *Transfer Learning* e para isso utilizou uma rede pré-disponibilizada pelo Pytorch, a Resnet152. Algumas modificações são feitas com o intuito de adequar a rede ao problema de classificação de 11 classes, sendo 10 referetens às espécies e 1 referente ao bioma do pantanal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsYmwqmK4FBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "4d362e81c33a4a9f946fc48b7f40e04c",
            "a424ad4ed20a47fc948f2e8642288fe7",
            "9d6e07adb1aa40579f68a43310d908eb",
            "1254a7235d2d49929ce2fbf993223a47",
            "5fb207a31b0f4a5385dcdbe67a47a541",
            "29194e4b97204342be6cd3a2cb40724e",
            "ec9a2e6283b444f1b5d8c5fb1b98fbb9",
            "015b2b7c44cb4f4d9f58d3f69f9a2fcd"
          ]
        },
        "outputId": "43f342d0-2dd7-4caa-b08a-7f2136b10834"
      },
      "source": [
        "# Fazemos o download da rede pre-treinada\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Modficamos a camada final para transformar as 2048 saídas da resnet em 11\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(1000,11),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Enviamos o modelo para a GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d362e81c33a4a9f946fc48b7f40e04c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRty-Na5oKwQ"
      },
      "source": [
        "A seguir são defenidos alguns parâmetros do modelo, sendo o *Learning Rate*, a função de *Loss*, NLL, e o otimizador AdamW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKRiBixe7e7o"
      },
      "source": [
        "lr = 0.0001\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.AdamW(model.fc.parameters(),lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERT9H8gVolK_"
      },
      "source": [
        "A função definida abaixo tem o intuito de treinar a rede. Os argumentos da função são o modelo que foi instanciado nas células acima (*model*), o dataset de treino (*loader_train*) e também o dataset de test (*loader_test*). A função tem como retorno a *loss* do treinamento do modelo, a *loss* referente ao dataset de teste e a acurácia do modelo referente ao dataset de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuunnu-K7pPR"
      },
      "source": [
        "def train_test(model, loader_train, loader_test):\n",
        "  train_loss, test_loss, acc = 0,0,0\n",
        "\n",
        "  # Modo de Treino\n",
        "  model.train()\n",
        "  training_loss = 0\n",
        "  for img, label in loader_train:\n",
        "    img, label = img.float().to(device), label.to(device)\n",
        "    loss = criterion(model(img), label.float().long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    training_loss += loss\n",
        "  train_loss = training_loss/len(loader_train)\n",
        "\n",
        "  # Modo de avaliação\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  testing_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for img, label in loader_test:\n",
        "      img, label = img.float().to(device), label.to(device)\n",
        "      out = model(img)\n",
        "      _, predict = torch.max(out, dim=1)\n",
        "      total += 1\n",
        "      if predict == label:\n",
        "        correct += 1\n",
        "      testing_loss += criterion(out, label.long())\n",
        "  test_loss = testing_loss/len(loader_test)\n",
        "  acc = (correct/total)*100\n",
        "\n",
        "  return train_loss, test_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFPdSsDp23x"
      },
      "source": [
        "## Treinamento\n",
        "A célula abaixo é responsável por realizar o treinamento do modelo. É definida a quantidade de épocas que o modelo será treinado e ao final de cada época são mostradas algumas informações, dentre elas a época, a loss da época, a acurácia e o tempo para a época. Aguarde 5 minutos durante a execução da célula pela primeira vez, caso não seja feito nenhum *print* reinicie a célula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBfUiLhW8z0_",
        "outputId": "2be53034-5f52-4156-9780-274e3888a350"
      },
      "source": [
        "START = time.time()\n",
        "time1epoch = 0\n",
        "\n",
        "# ========== Epocas ==========#\n",
        "epochs = 15\n",
        "# ============================\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  start = time.time()\n",
        "  train_loss, test_loss, acc = train_test(model, loader_train, loader_test)\n",
        "  end = time.time()\n",
        "\n",
        "  Time = end - start\n",
        "  if epoch == 1:\n",
        "    time1epoch = Time\n",
        "\n",
        "  print('Epoch: ', epoch ,' loss: {:.4f}'.format(test_loss.item()), ' Accuracy: {:.2f}'.format(acc), ' Time spent this epoch: {:.2f}'.format(Time), 'seconds.')\n",
        "END = time.time()\n",
        "\n",
        "print()\n",
        "TIME = (END - START) - time1epoch\n",
        "print('\\n Time spent during training, excluding first epoch: {:.2f}'.format(TIME), 'seconds.')\t\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1  loss: 0.2211  Accuracy: 98.19  Time spent this epoch: 21.24 seconds.\n",
            "Epoch:  2  loss: 0.1884  Accuracy: 97.41  Time spent this epoch: 21.44 seconds.\n",
            "Epoch:  3  loss: 0.1590  Accuracy: 97.15  Time spent this epoch: 20.84 seconds.\n",
            "Epoch:  4  loss: 0.1288  Accuracy: 97.67  Time spent this epoch: 20.65 seconds.\n",
            "Epoch:  5  loss: 0.1178  Accuracy: 98.70  Time spent this epoch: 20.86 seconds.\n",
            "Epoch:  6  loss: 0.0996  Accuracy: 98.45  Time spent this epoch: 21.01 seconds.\n",
            "Epoch:  7  loss: 0.0979  Accuracy: 98.45  Time spent this epoch: 20.94 seconds.\n",
            "Epoch:  8  loss: 0.0987  Accuracy: 98.19  Time spent this epoch: 20.87 seconds.\n",
            "Epoch:  9  loss: 0.0994  Accuracy: 97.41  Time spent this epoch: 20.96 seconds.\n",
            "Epoch:  10  loss: 0.0863  Accuracy: 97.93  Time spent this epoch: 20.94 seconds.\n",
            "Epoch:  11  loss: 0.0854  Accuracy: 97.93  Time spent this epoch: 20.89 seconds.\n",
            "Epoch:  12  loss: 0.0783  Accuracy: 98.45  Time spent this epoch: 20.91 seconds.\n",
            "Epoch:  13  loss: 0.0835  Accuracy: 98.19  Time spent this epoch: 20.95 seconds.\n",
            "Epoch:  14  loss: 0.0874  Accuracy: 97.93  Time spent this epoch: 20.91 seconds.\n",
            "Epoch:  15  loss: 0.0827  Accuracy: 98.45  Time spent this epoch: 20.90 seconds.\n",
            "Epoch:  16  loss: 0.0727  Accuracy: 98.70  Time spent this epoch: 20.87 seconds.\n",
            "Epoch:  17  loss: 0.0775  Accuracy: 98.19  Time spent this epoch: 20.89 seconds.\n",
            "Epoch:  18  loss: 0.0745  Accuracy: 97.15  Time spent this epoch: 20.87 seconds.\n",
            "Epoch:  19  loss: 0.0828  Accuracy: 98.19  Time spent this epoch: 20.89 seconds.\n",
            "Epoch:  20  loss: 0.0746  Accuracy: 97.93  Time spent this epoch: 20.94 seconds.\n",
            "Epoch:  21  loss: 0.0780  Accuracy: 97.41  Time spent this epoch: 20.94 seconds.\n",
            "Epoch:  22  loss: 0.0680  Accuracy: 98.19  Time spent this epoch: 20.89 seconds.\n",
            "Epoch:  23  loss: 0.0691  Accuracy: 98.19  Time spent this epoch: 20.85 seconds.\n",
            "Epoch:  24  loss: 0.0732  Accuracy: 98.70  Time spent this epoch: 20.87 seconds.\n",
            "Epoch:  25  loss: 0.0682  Accuracy: 98.70  Time spent this epoch: 20.89 seconds.\n",
            "Epoch:  26  loss: 0.0675  Accuracy: 97.41  Time spent this epoch: 20.88 seconds.\n",
            "Epoch:  27  loss: 0.0777  Accuracy: 97.67  Time spent this epoch: 20.94 seconds.\n",
            "Epoch:  28  loss: 0.0627  Accuracy: 98.45  Time spent this epoch: 20.87 seconds.\n",
            "Epoch:  29  loss: 0.0677  Accuracy: 98.19  Time spent this epoch: 20.88 seconds.\n",
            "Epoch:  30  loss: 0.0738  Accuracy: 97.93  Time spent this epoch: 20.95 seconds.\n",
            "\n",
            "\n",
            " Time spent during training, excluding first epoch: 606.50 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhuDWbmHq4hB"
      },
      "source": [
        "## Validação\n",
        "Com o intuito de obter uma outra forma de avaliar o modelo, é utilizado o dataset de validação para obter os valores de acurácia para imagens que não foram vistas pelo modelo. São dispostos os valores da acurácia para o dataset de validação, o erro médio e também o tempo gasto durante o treinamento, excluindo a primeira época."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Ump2di-ZXQ",
        "outputId": "cafb2fc4-f1c3-4d8a-9442-c03b58ad3381"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "loss_valid = 0\n",
        "\n",
        "for imagem_valid, label_valid in loader_valid:\n",
        "\n",
        "\tloss_v = 0\n",
        "\n",
        "\timagem_valid, label_valid = imagem_valid.float().to(device), label_valid.to(device)\n",
        "\toutputs_valid = model(imagem_valid)\n",
        "\t_, previsao = torch.max(outputs_valid, dim = 1)\n",
        "\tloss_v = criterion (outputs_valid, label_valid.long())\n",
        "\ttotal = total + 1 #Adiciona +1 na variável que guarda o total de previsões feitas\n",
        "\tif previsao == label_valid: \n",
        "\t\tcorrect = correct + 1 #Soma +1 na variável que mede quantas previsões dessa categoria (erro absoluto de 2 graus) estão certas\n",
        "\tloss_valid += loss_v\n",
        "\n",
        "loss_valid = loss_valid/len(loader_valid)\n",
        "accuracy = (correct/total)*100 #Calcula a acurácia para erro absoluto de 1 grau em porcentagem\n",
        "\n",
        "print('A acurácia obtida foi de: {:.2f}'.format(accuracy) + '%.')\n",
        "print()\n",
        "print('O erro médio obtido foi de: {:.4f}'.format(loss_valid))\n",
        "print()\n",
        "print('O tempo gasto durante o treinamento, excluindo a primeira época, foi de: {:.2f}'.format(TIME), 'segundos.')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A acurácia obtida foi de: 97.67%.\n",
            "\n",
            "O erro médio obtido foi de: 0.0694\n",
            "\n",
            "O tempo gasto durante o treinamento, excluindo a primeira época, foi de: 606.50 segundos.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT4sLlmZsWxv"
      },
      "source": [
        "## Predict\n",
        "A seguir será implementada a função *predict* responsável por retornar dois valores, o primeiro é um número que faz referência a qual das 11 classes foi identificada na imagem e o segundo valor pode assumir três valores distintos dependendo se a espécie em questão está em extinção, se a espécie não está em extinção ou até mesmo se na imagem não está presente nenhuma das espécies listadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNWO6gl0YlK"
      },
      "source": [
        "def predict(image_tensor, model):\n",
        "  '''\n",
        "  0 - onça-pintada\n",
        "  1 - lobo guará\n",
        "  2 - ariranha\n",
        "  3 - tatu canastra\n",
        "  4 - tamanduá bandeira\n",
        "  5 - jacaré do papo amarelo\n",
        "  6 - sucuri\n",
        "  7 - tucano\n",
        "  8 - piranha\n",
        "  9 - capivara\n",
        "  10 - pantanal\n",
        "  '''\n",
        "  dic_aux = {0:2, 1:9, 2:5, 3:1, 4:0, 5:10, 6:8, 7:6, 8:4, 9:3, 10:7}\n",
        "  '''\n",
        "  O dicionário utilizado aqui tem como intuito adequar os índices de cada um dos\n",
        "  animais com os que foram disponibilizados no roteiro do desafio final.\n",
        "  Dessa forma os valores de data.class_to_idx são convertidos nos indíces seguindo\n",
        "  o que foi proposto no roteiro.\n",
        "  '''\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(image_tensor)\n",
        "    _, predict = torch.max(out, dim=1)\n",
        "    predict = predict.item()\n",
        "    predict_animal = dic_aux[predict]\n",
        "    if predict_animal < 5:\n",
        "      conteudo_imagem = 'a'\n",
        "    elif predict_animal < 10:\n",
        "      conteudo_imagem = 'b'\n",
        "    elif predict_animal == 10:\n",
        "      conteudo_imagem = 'c'\n",
        "    return predict_animal, conteudo_imagem\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsNe0YWUfZ2X"
      },
      "source": [
        "## Salvando o modelo\n",
        "A célula abaixo salva o modelo no formato .pt na pasta IMLDL do google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNWFKSzjkYB"
      },
      "source": [
        "torch.save(model, '/content/drive/My Drive/IMLDL/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}